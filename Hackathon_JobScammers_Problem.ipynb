{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e0197365-082b-44c7-b22a-d4f297407831",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\shubh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates: 181\n",
      "Shape after removing duplicates: (14123, 17)\n",
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 14123 entries, 0 to 14302\n",
      "Data columns (total 17 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   title                14123 non-null  object\n",
      " 1   location             13845 non-null  object\n",
      " 2   department           4988 non-null   object\n",
      " 3   salary_range         2258 non-null   object\n",
      " 4   company_profile      11465 non-null  object\n",
      " 5   description          14122 non-null  object\n",
      " 6   requirements         12012 non-null  object\n",
      " 7   benefits             8380 non-null   object\n",
      " 8   telecommuting        14123 non-null  int64 \n",
      " 9   has_company_logo     14123 non-null  int64 \n",
      " 10  has_questions        14123 non-null  int64 \n",
      " 11  employment_type      11392 non-null  object\n",
      " 12  required_experience  8499 non-null   object\n",
      " 13  required_education   7677 non-null   object\n",
      " 14  industry             10234 non-null  object\n",
      " 15  function             9007 non-null   object\n",
      " 16  fraudulent           14123 non-null  int64 \n",
      "dtypes: int64(4), object(13)\n",
      "memory usage: 1.9+ MB\n",
      "None\n",
      "\n",
      "First 5 rows:\n",
      "                             title                location department  \\\n",
      "0   Contact Center Representatives  US, VA, Virginia Beach        NaN   \n",
      "1      Customer Service Associate           US, TX, Dallas        NaN   \n",
      "2           Automated Test Analyst         NZ, , Auckland   Permanent   \n",
      "3  Inside Sales Professional-Omaha           US, NE, Omaha        NaN   \n",
      "4    Content Marketing/SEO Manager     US, CA, Los Angeles  Marketing   \n",
      "\n",
      "  salary_range                                    company_profile  \\\n",
      "0          NaN  Tidewater Finance Co. was established in 1992 ...   \n",
      "1          NaN  Novitex Enterprise Solutions, formerly Pitney ...   \n",
      "2          NaN  SilverStripe CMS &amp; Framework is an open so...   \n",
      "3          NaN  ABC Supply Co., Inc. is the nationâ€™s largest w...   \n",
      "4          NaN  MeUndies is a lifestyle brand that is transfor...   \n",
      "\n",
      "                                         description  \\\n",
      "0  Tidewater Finance Company, located in Virginia...   \n",
      "1  The Customer Service Associate will be based i...   \n",
      "2  We are looking for a dedicated and passionate ...   \n",
      "3  As a Sales Representative, you will provide as...   \n",
      "4  MeUndies is a lifestyle brand that is transfor...   \n",
      "\n",
      "                                        requirements  \\\n",
      "0  The position requires the following qualificat...   \n",
      "1  QualificationsMinimum of 1 year customer servi...   \n",
      "2                                                NaN   \n",
      "3  As a Sales Representative, you must have the a...   \n",
      "4  REQUIREMENTS/QUALIFICATIONS/PERSONAL ATTRIBUTE...   \n",
      "\n",
      "                                            benefits  telecommuting  \\\n",
      "0  Our company offers a competitive salary plus B...              0   \n",
      "1                                                NaN              0   \n",
      "2                                                NaN              0   \n",
      "3  Your benefits package as a Sales Representativ...              0   \n",
      "4  WHY MEUNDIES?We're a fast-growing, VC-backed c...              0   \n",
      "\n",
      "   has_company_logo  has_questions employment_type required_experience  \\\n",
      "0                 1              0       Full-time         Entry level   \n",
      "1                 1              0       Full-time         Entry level   \n",
      "2                 1              1       Full-time    Mid-Senior level   \n",
      "3                 1              0       Full-time                 NaN   \n",
      "4                 1              0       Full-time    Mid-Senior level   \n",
      "\n",
      "          required_education                             industry  \\\n",
      "0                Unspecified                   Financial Services   \n",
      "1  High School or equivalent                   Telecommunications   \n",
      "2                        NaN  Information Technology and Services   \n",
      "3                        NaN                   Building Materials   \n",
      "4          Bachelor's Degree                             Internet   \n",
      "\n",
      "           function  fraudulent  \n",
      "0  Customer Service           0  \n",
      "1  Customer Service           0  \n",
      "2               NaN           0  \n",
      "3             Sales           0  \n",
      "4         Marketing           0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "# Load the training dataset\n",
    "df = pd.read_csv('Train_data_Job.csv') \n",
    "\n",
    "# Drop irrelevant column\n",
    "df = df.drop(columns=['job_id'])\n",
    "# Remove duplicates\n",
    "print(\"Number of duplicates:\", df.duplicated().sum())\n",
    "df = df.drop_duplicates()\n",
    "print(\"Shape after removing duplicates:\", df.shape)\n",
    "# Display basic info to verify loading\n",
    "print(\"Dataset Info:\")\n",
    "print(df.info())\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e08095b6-8857-4739-bef7-077a28893de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution (Fraudulent vs Genuine):\n",
      "fraudulent\n",
      "0    0.951285\n",
      "1    0.048715\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Check class distribution\n",
    "print(\"Class Distribution (Fraudulent vs Genuine):\")\n",
    "print(df['fraudulent'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "926153d0-5ff9-4c37-8f5e-b19c2eb032c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values After Handling:\n",
      "title                      0\n",
      "location                   0\n",
      "department                 0\n",
      "salary_range           11865\n",
      "company_profile            0\n",
      "description                0\n",
      "requirements               0\n",
      "benefits                   0\n",
      "telecommuting              0\n",
      "has_company_logo           0\n",
      "has_questions              0\n",
      "employment_type            0\n",
      "required_experience        0\n",
      "required_education         0\n",
      "industry                   0\n",
      "function                   0\n",
      "fraudulent                 0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Fill missing values for text columns\n",
    "text_columns = ['title', 'company_profile', 'description', 'requirements', 'benefits']\n",
    "for col in text_columns:\n",
    "    df[col] = df[col].fillna(\"Not Provided\")\n",
    "\n",
    "# Fill missing values for categorical columns\n",
    "categorical_columns = ['location', 'department', 'employment_type', 'required_experience', 'required_education', 'industry', 'function']\n",
    "for col in categorical_columns:\n",
    "    df[col] = df[col].fillna(\"Unknown\")\n",
    "# Verify missing values are handled\n",
    "print(\"Missing Values After Handling:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2bec1f09-dcc8-4206-bb5c-a93b4b39f623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Classification Report (Threshold = 0.2):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Genuine       0.99      0.99      0.99      2687\n",
      "  Fraudulent       0.80      0.85      0.82       138\n",
      "\n",
      "    accuracy                           0.98      2825\n",
      "   macro avg       0.90      0.92      0.91      2825\n",
      "weighted avg       0.98      0.98      0.98      2825\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['tfidf_title_revised.pkl']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Enhanced text cleaning\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def advanced_preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)  # Remove URLs\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))  # Remove punctuation\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Apply to text columns separately\n",
    "df['description_clean'] = df['description'].apply(advanced_preprocess_text)\n",
    "df['company_profile_clean'] = df['company_profile'].apply(advanced_preprocess_text)\n",
    "df['title_clean'] = df['title'].apply(advanced_preprocess_text)\n",
    "\n",
    "# TF-IDF for each text column\n",
    "tfidf_desc = TfidfVectorizer(max_features=3000)\n",
    "tfidf_comp = TfidfVectorizer(max_features=1000)\n",
    "tfidf_title = TfidfVectorizer(max_features=500)\n",
    "X_desc = tfidf_desc.fit_transform(df['description_clean'])\n",
    "X_comp = tfidf_comp.fit_transform(df['company_profile_clean'])\n",
    "X_title = tfidf_title.fit_transform(df['title_clean'])\n",
    "\n",
    "# Process salary_range\n",
    "df['has_salary_range'] = df['salary_range'].notnull().astype(int)\n",
    "def parse_salary(salary):\n",
    "    if pd.isna(salary):\n",
    "        return 0  # Impute with 0 instead of median\n",
    "    try:\n",
    "        salary = salary.replace('$', '').replace(',', '')\n",
    "        low, high = salary.split('-')\n",
    "        return (float(low) + float(high)) / 2\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "df['salary_avg'] = df['salary_range'].apply(parse_salary)\n",
    "\n",
    "# Target encoding for categorical features\n",
    "cat_features = ['employment_type', 'required_experience', 'industry']\n",
    "for col in cat_features:\n",
    "    fraud_rate = df.groupby(col)['fraudulent'].mean()\n",
    "    df[f'{col}_target_enc'] = df[col].map(fraud_rate)\n",
    "\n",
    "# Add fraud-specific features\n",
    "df['desc_length'] = df['description'].apply(len)\n",
    "#df['urgent_flag'] = df['description'].str.contains('urgent|immediate', case=False, na=False).astype(int)\n",
    "df['urgent_flag'] = df['description'].str.contains('urgent|immediate|asap|now|pressing|hurry|limited time', case=False, na=False).astype(int)\n",
    "# Combine all features\n",
    "binary_features = ['telecommuting', 'has_company_logo', 'has_questions', 'has_salary_range']\n",
    "X_binary = df[binary_features].values\n",
    "X_extra = df[['salary_avg', 'desc_length', 'urgent_flag', 'employment_type_target_enc', 'required_experience_target_enc', 'industry_target_enc']].values\n",
    "X = np.hstack((X_desc.toarray(), X_comp.toarray(), X_title.toarray(), X_binary, X_extra))\n",
    "y = df['fraudulent'].values\n",
    "\n",
    "# Apply SMOTE to training set\n",
    "X_temp, X_val, y_temp, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train, y_train = smote.fit_resample(X_temp, y_temp)\n",
    "\n",
    "# Train XGBoost with scale_pos_weight\n",
    "scale_pos_weight = (len(y_train) - sum(y_train)) / sum(y_train)\n",
    "xgb_model = XGBClassifier(scale_pos_weight=scale_pos_weight, random_state=42, eval_metric='logloss')\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate with fixed threshold of 0.2\n",
    "y_pred_proba_xgb = xgb_model.predict_proba(X_val)[:, 1]\n",
    "y_pred_xgb = (y_pred_proba_xgb >= 0.2).astype(int)\n",
    "print(\"XGBoost Classification Report (Threshold = 0.2):\")\n",
    "print(classification_report(y_val, y_pred_xgb, target_names=['Genuine', 'Fraudulent']))\n",
    "\n",
    "# Save model and preprocessors\n",
    "joblib.dump(xgb_model, 'job_fraud_xgb_revised_threshold_0_2.pkl')\n",
    "joblib.dump(tfidf_desc, 'tfidf_desc_revised.pkl')\n",
    "joblib.dump(tfidf_comp, 'tfidf_comp_revised.pkl')\n",
    "joblib.dump(tfidf_title, 'tfidf_title_revised.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ec5e2270-bc94-40bb-abb5-6231ea0679f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test predictions saved to 'test_predictions_xgb_final.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Load the test dataset\n",
    "df_test = pd.read_csv('Test_data_Job.csv')\n",
    "\n",
    "# Drop irrelevant column\n",
    "df_test = df_test.drop(columns=['job_id'])\n",
    "\n",
    "# Remove duplicates\n",
    "df_test = df_test.drop_duplicates()\n",
    "\n",
    "# Fill missing values for text columns\n",
    "text_columns = ['title', 'company_profile', 'description', 'requirements', 'benefits']\n",
    "for col in text_columns:\n",
    "    df_test[col] = df_test[col].fillna(\"Not Provided\")\n",
    "\n",
    "# Fill missing values for categorical columns\n",
    "categorical_columns = ['location', 'department', 'employment_type', 'required_experience', 'required_education', 'industry', 'function']\n",
    "for col in categorical_columns:\n",
    "    df_test[col] = df_test[col].fillna(\"Unknown\")\n",
    "\n",
    "# Apply the same text preprocessing\n",
    "df_test['description_clean'] = df_test['description'].apply(advanced_preprocess_text)\n",
    "df_test['company_profile_clean'] = df_test['company_profile'].apply(advanced_preprocess_text)\n",
    "df_test['title_clean'] = df_test['title'].apply(advanced_preprocess_text)\n",
    "\n",
    "# Transform using the same TF-IDF vectorizers\n",
    "X_desc_test = tfidf_desc.transform(df_test['description_clean'])\n",
    "X_comp_test = tfidf_comp.transform(df_test['company_profile_clean'])\n",
    "X_title_test = tfidf_title.transform(df_test['title_clean'])\n",
    "\n",
    "# Process salary_range\n",
    "df_test['has_salary_range'] = df_test['salary_range'].notnull().astype(int)\n",
    "df_test['salary_avg'] = df_test['salary_range'].apply(parse_salary)\n",
    "\n",
    "# Target encoding for categorical features (using training set mappings)\n",
    "for col in cat_features:\n",
    "    df_test[f'{col}_target_enc'] = df_test[col].map(fraud_rate)  # fraud_rate from training set\n",
    "    df_test[f'{col}_target_enc'] = df_test[f'{col}_target_enc'].fillna(fraud_rate.mean())  # Handle unseen categories\n",
    "\n",
    "# Add fraud-specific features\n",
    "df_test['desc_length'] = df_test['description'].apply(len)\n",
    "df_test['urgent_flag'] = df_test['description'].str.contains('urgent|immediate|asap|now|pressing|hurry|limited time', case=False, na=False).astype(int)\n",
    "\n",
    "# Combine all features\n",
    "X_binary_test = df_test[binary_features].values\n",
    "X_extra_test = df_test[['salary_avg', 'desc_length', 'urgent_flag', 'employment_type_target_enc', 'required_experience_target_enc', 'industry_target_enc']].values\n",
    "X_test = np.hstack((X_desc_test.toarray(), X_comp_test.toarray(), X_title_test.toarray(), X_binary_test, X_extra_test))\n",
    "\n",
    "\n",
    "# Predict on test set using XGBoost\n",
    "y_pred_test = (xgb_model.predict_proba(X_test)[:, 1] >= 0.2).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "# Save test predictions\n",
    "df_test['Prediction'] = y_pred_test\n",
    "df_test['Fraud_Probability'] = xgb_model.predict_proba(X_test)[:, 1]\n",
    "df_test.to_csv('test_predictions_xgb_final.csv', index=False)\n",
    "print(\"Test predictions saved to 'test_predictions_xgb_final.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2e3aba17-dbc6-423e-a3e9-3ba6b2eb5d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total test samples: 3564\n",
      "Predicted fraudulent postings: 207\n",
      "Proportion predicted as fraudulent: 0.05808080808080808\n"
     ]
    }
   ],
   "source": [
    "print(\"Total test samples:\", len(df_test))\n",
    "print(\"Predicted fraudulent postings:\", df_test['Prediction'].sum())\n",
    "print(\"Proportion predicted as fraudulent:\", df_test['Prediction'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6f63c7bf-df88-49de-a086-d8eefee28d1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fraud_rate.pkl']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraud_rate = {}\n",
    "cat_features = ['employment_type', 'required_experience', 'industry']\n",
    "for col in cat_features:\n",
    "    fraud_rate[col] = df.groupby(col)['fraudulent'].mean()\n",
    "joblib.dump(fraud_rate, 'fraud_rate.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e34fbac-620b-44a9-8c4a-1e20c1363c90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
